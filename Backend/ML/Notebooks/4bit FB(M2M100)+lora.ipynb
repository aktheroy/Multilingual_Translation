{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Third-party library imports\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from evaluate import load as load_metric  # Renamed for clarity when loading metrics\n",
    "from matplotlib import pyplot as plt  # Fixed incorrect alias\n",
    "\n",
    "# Transformers and related libraries\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_data_select = \"train[:100%]\" # add percent sign ie. \"train[:20%]\" to select that percent of data \n",
    "# Load only 20% of the dataset\n",
    "dataset = load_dataset(\n",
    "    \"csv\", data_files={\"train\": \"../Datasets/WikiMatrix/Processed/clean_en-hi.csv\"},\n",
    "    split=percent_data_select\n",
    ")\n",
    "\n",
    "# Split into train and test sets (e.g., 80% train, 20% test)\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Further split the test set into validation and test (e.g., 50-50 split of the 20%)\n",
    "validation_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Combine splits into a DatasetDict\n",
    "raw_dataset = {\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"validation\": validation_test_split[\"train\"],\n",
    "    \"test\": validation_test_split[\"test\"]\n",
    "}\n",
    "\n",
    "dataset = DatasetDict(raw_dataset)\n",
    "\n",
    "# Inspect the resulting dataset\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - `percent_data_select = \"train[:100%]\"`: This step determines the proportion of data to load. By specifying a percentage (e.g., `\"train[:20%]\"`), you are sampling a subset of the data from the entire dataset.\n",
    "   - This is useful for:\n",
    "     - Reducing computational load during development or testing.\n",
    "     - Experimenting with smaller portions of the data.\n",
    "     \n",
    "**Dataset Splitting**\n",
    "   - `train_test_split`: This step divides the dataset into **training** and **test** sets, often using an 80-20 split. The `seed` parameter ensures reproducibility by fixing the randomization.\n",
    "   - `validation_test_split`: Further splits the test set into **validation** and **test** sets, typically ensuring the final dataset structure is:\n",
    "     - Training Set: 80%\n",
    "     - Validation Set: 10%\n",
    "     - Test Set: 10%\n",
    "\n",
    "**Combining into a DatasetDict**\n",
    "   - The `DatasetDict` organizes these splits (`train`, `validation`, `test`) into a cohesive structure. This is a common practice when working with Hugging Face's `datasets` library, as it standardizes access to each split.\n",
    "\n",
    "**Inspection**\n",
    "   - `print(dataset)`: Displays the dataset structure, providing details about the splits, number of samples, and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty VRAM cache\n",
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Configuring 4-bit Quantization for Efficient Model Loading**\n",
    "\n",
    "* This code configures and loads a pre-trained sequence-to-sequence model with 4-bit quantization, which is a technique used to reduce memory usage and improve inference speed without significant loss in model accuracy. Here's a detailed explanation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Enable 4-bit quantization\n",
    "    bnb_4bit_use_double_quant=True,  # Use double quantization for better accuracy\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Use 4-bit NormalFloat quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Use FP16 for computation\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"facebook/m2m100_418M\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/m2m100_418M', src_lang=\"en\", tgt_lang=\"hi\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"4&16&32 bits.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "\n",
    "**1. Quantization Configuration**\n",
    "The `BitsAndBytesConfig` object defines the settings for 4-bit quantization:\n",
    "- **`load_in_4bit=True`**:\n",
    "  - Enables loading the model weights in 4-bit precision, reducing memory usage compared to standard 16-bit (FP16) or 32-bit (FP32) formats.\n",
    "  \n",
    "- **`bnb_4bit_use_double_quant=True`**:\n",
    "  - Activates double quantization, which improves quantization accuracy by first applying an intermediate quantization step.\n",
    "\n",
    "- **`bnb_4bit_quant_type=\"nf4\"`**:\n",
    "  - Specifies the quantization type as **NormalFloat4 (NF4)**, a more efficient quantization method that maintains numerical precision better than standard integer quantization.\n",
    "\n",
    "- **`bnb_4bit_compute_dtype=torch.float16`**:\n",
    "  - Sets the computation to use FP16 (16-bit floating-point), balancing performance and accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Model Loading**\n",
    "The model is loaded using `AutoModelForSeq2SeqLM.from_pretrained`, where:\n",
    "- **`facebook/m2m100_418M`**:\n",
    "  - The name of the pre-trained multilingual model designed for translation tasks.\n",
    "- **`quantization_config=quantization_config`**:\n",
    "  - Applies the 4-bit quantization settings during model loading, enabling reduced VRAM usage while maintaining effective performance.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Tokenizer Setup**\n",
    "The tokenizer is initialized for the same model:\n",
    "- **`src_lang=\"en\"`** and **`tgt_lang=\"hi\"`**:\n",
    "  - Specifies English as the source language and Hindi as the target language for translation.\n",
    "\n",
    "---\n",
    "\n",
    "**Why Use 4-bit Quantization?**\n",
    "- **Memory Efficiency**: Reduces the size of the model, making it possible to run on GPUs with limited VRAM.\n",
    "- **Faster Inference**: Smaller models require less computation, speeding up translation tasks.\n",
    "- **Retained Accuracy**: Advanced quantization techniques (e.g., NF4) minimize accuracy loss during model optimization.\n",
    "\n",
    "This configuration is particularly useful when deploying large models on resource-constrained hardware or optimizing for inference speed in production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing Data for Translation Tasks**\n",
    "\n",
    "This code defines a preprocessing function and applies it to a dataset for preparing data suitable for a machine translation task using a sequence-to-sequence (Seq2Seq) model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, src_lang, tgt_lang):\n",
    "    inputs = [f\"translate {src_lang} to {tgt_lang}: \" + ex for ex in examples[src_lang]]\n",
    "    targets = examples[tgt_lang]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets_english_to_hindi = dataset.map(lambda x: preprocess_function(x, \"English\", \"Hindi\"), batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing Function**\n",
    "The `preprocess_function` processes raw examples from the dataset into a format required by the tokenizer and model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Applying LoRA (Low-Rank Adaptation) to a Seq2Seq Model**\n",
    "\n",
    "This code configures and applies **Low-Rank Adaptation (LoRA)** to a sequence-to-sequence (Seq2Seq) model. LoRA is a parameter-efficient fine-tuning technique that reduces the number of trainable parameters while maintaining model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank of the low-rank matrices\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Target specific layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"  # Task type for sequence-to-sequence models\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LoRA Configuration**\n",
    "The `LoraConfig` object defines the key parameters for applying LoRA:\n",
    "\n",
    "- **`r=16`**:\n",
    "  - Specifies the rank of the low-rank matrices used for adaptation. A lower rank reduces the number of trainable parameters.\n",
    "\n",
    "- **`lora_alpha=32`**:\n",
    "  - A scaling factor that adjusts the influence of the LoRA modules on the model's performance.\n",
    "\n",
    "- **`target_modules=[\"q_proj\", \"v_proj\"]`**:\n",
    "  - Specifies which layers of the model should be adapted. Here, the attention layers' query (`q_proj`) and value (`v_proj`) projections are targeted.\n",
    "\n",
    "- **`lora_dropout=0.1`**:\n",
    "  - Introduces a dropout rate of 10% to prevent overfitting during training.\n",
    "\n",
    "- **`bias=\"none\"`**:\n",
    "  - Indicates that no bias terms will be trained or adapted in the LoRA modules.\n",
    "\n",
    "- **`task_type=\"SEQ_2_SEQ_LM\"`**:\n",
    "  - Specifies the task type as sequence-to-sequence language modeling, ensuring compatibility with the model architecture.\n",
    "\n",
    "---\n",
    "**Applying LoRA to the Model**\n",
    "```python\n",
    "model = get_peft_model(model, lora_config)\n",
    "```\n",
    "- **`get_peft_model`**:\n",
    "  - Modifies the existing model by injecting LoRA layers into the specified `target_modules`.\n",
    "  - This makes the model efficient for fine-tuning by freezing the original model weights and learning only the low-rank adaptation parameters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Why Use LoRA?**\n",
    "1. **Parameter Efficiency**:\n",
    "   - Instead of fine-tuning all model parameters, only a small set of low-rank matrices is trained, drastically reducing memory and computational costs.\n",
    "\n",
    "2. **Scalability**:\n",
    "   - LoRA enables fine-tuning of large models on hardware with limited resources.\n",
    "\n",
    "3. **Task-Specific Adaptation**:\n",
    "   - LoRA allows for task-specific tuning without altering the base model, making it ideal for transfer learning.\n",
    "\n",
    "---\n",
    "\n",
    "This setup is well-suited for training Seq2Seq models like translation, summarization, or text generation tasks on resource-constrained hardware while maintaining competitive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Collator for Sequence-to-Sequence Tasks**\n",
    "- The provided code creates a DataCollatorForSeq2Seq object, which is used to efficiently prepare batches of data for sequence-to-sequence (Seq2Seq) models during training or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training a Seq2Seq Model with `Seq2SeqTrainer`**\n",
    "\n",
    "This code sets up and trains a sequence-to-sequence (Seq2Seq) model using Hugging Face's `Seq2SeqTrainer`, a convenient API designed for tasks like translation, summarization, and other text-to-text tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",  # Save after each epoch (match evaluation strategy)\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=500,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    lr_scheduler_type=\"linear\",  # Linear decay after warmup\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    predict_with_generate=True,\n",
    "    report_to=None,  # Or \"wandb\" if integrated\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets_english_to_hindi[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_english_to_hindi[\"validation\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Training Arguments**\n",
    "The `Seq2SeqTrainingArguments` defines hyperparameters and configurations for training. Here’s a breakdown of its key components:\n",
    "\n",
    "- **General Training Configuration**:\n",
    "  - **`output_dir=\"./results\"`**: Specifies the directory to save model checkpoints and logs.\n",
    "  - **`do_train=True`**: Enables training.\n",
    "  - **`do_eval=True`**: Enables evaluation.\n",
    "\n",
    "- **Evaluation and Checkpointing**:\n",
    "  - **`evaluation_strategy=\"epoch\"`**: Evaluates the model at the end of every epoch.\n",
    "  - **`save_strategy=\"epoch\"`**: Saves model checkpoints at the end of every epoch.\n",
    "  - **`metric_for_best_model=\"eval_loss\"`**: Monitors evaluation loss to determine the best model.\n",
    "\n",
    "- **Hyperparameters**:\n",
    "  - **`num_train_epochs=10`**: Trains the model for 10 epochs.\n",
    "  - **`learning_rate=2e-5`**: Sets the initial learning rate for the optimizer.\n",
    "  - **`warmup_steps=500`**: Gradually increases the learning rate for the first 500 steps to stabilize training.\n",
    "  - **`weight_decay=0.1`**: Applies weight decay to regularize the model.\n",
    "\n",
    "- **Batching and Gradient Updates**:\n",
    "  - **`per_device_train_batch_size=8`**: Sets the training batch size per GPU.\n",
    "  - **`per_device_eval_batch_size=8`**: Sets the evaluation batch size per GPU.\n",
    "  - **`gradient_accumulation_steps=4`**: Accumulates gradients over 4 steps before updating model weights, effectively increasing the batch size.\n",
    "\n",
    "- **Optimization**:\n",
    "  - **`fp16=True`**: Enables mixed-precision training for faster computation and reduced memory usage.\n",
    "  - **`lr_scheduler_type=\"linear\"`**: Uses a linear learning rate decay after the warmup phase.\n",
    "\n",
    "- **Logging**:\n",
    "  - **`logging_steps=10`**: Logs training metrics every 10 steps.\n",
    "\n",
    "- **Reporting**:\n",
    "  - **`report_to=None`**: Disables integration with tracking tools like Weights & Biases. Use `\"wandb\"` if needed.\n",
    "\n",
    "- **Prediction**:\n",
    "  - **`predict_with_generate=True`**: Ensures the model generates sequences (e.g., translations) during evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Training the Model**\n",
    "The `Seq2SeqTrainer` is initialized with the following components:\n",
    "- **`model=model`**: The Seq2Seq model with LoRA applied.\n",
    "- **`args=training_args`**: The training configuration defined above.\n",
    "- **`data_collator=data_collator`**: Handles padding and formatting of inputs for training batches.\n",
    "- **`train_dataset=tokenized_datasets_english_to_hindi[\"train\"]`**: Specifies the training dataset.\n",
    "- **`eval_dataset=tokenized_datasets_english_to_hindi[\"validation\"]`**: Specifies the validation dataset.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Training Execution**\n",
    "```python\n",
    "trainer.train()\n",
    "```\n",
    "- This method begins training based on the defined arguments and datasets.\n",
    "- Outputs:\n",
    "  - Training and evaluation metrics after each epoch.\n",
    "  - Checkpoints saved in `output_dir` after each epoch.\n",
    "  - The best-performing model based on the `metric_for_best_model`.\n",
    "\n",
    "---\n",
    "\n",
    "**Why Use `Seq2SeqTrainer`?**\n",
    "- **Simplifies Workflow**: Abstracts away much of the boilerplate for training Seq2Seq models.\n",
    "- **Flexible Configuration**: Easily integrates LoRA, mixed precision, and custom evaluation strategies.\n",
    "- **Scalable**: Supports distributed training and gradient accumulation for large models.\n",
    "\n",
    "This setup is ideal for tasks like machine translation (English-to-Hindi in this case) while optimizing resource usage and training efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Save Model and Tokenizer Locally**\n",
    "```python\n",
    "trainer.save_model(\"../Model/lora/M2M100/\")\n",
    "tokenizer.save_pretrained(\"../Model/lora/M2M100/\")\n",
    "```\n",
    "- **Purpose**:\n",
    "  - Saves the fine-tuned model weights, configuration, and tokenizer for reuse.\n",
    "- **Path**:\n",
    "  - The model and tokenizer are saved in `../Model/lora/M2M100/`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"../Model/lora/M2M100/\")\n",
    "tokenizer.save_pretrained(\"../Model/lora/M2M100/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Translation with the Saved Model**\n",
    "```python\n",
    "text = 'break a leg'\n",
    "translator = pipeline(\"translation_en_to_hi\", model=\"../Model/lora/M2M100/\")\n",
    "translator(text)\n",
    "```\n",
    "- **Translation Pipeline**:\n",
    "  - Loads the saved model using Hugging Face's `pipeline` for English-to-Hindi translation.\n",
    "- **Input**:\n",
    "  - `text = 'break a leg'`: A common English idiom.\n",
    "- **Output**:\n",
    "  - The translated text in Hindi.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'break a leg'\n",
    "translator = pipeline(\"translation_en_to_hi\", model=\"../Model/lora/M2M100/\")\n",
    "translator(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Push Model to Hugging Face Hub**\n",
    "```python\n",
    "model.push_to_hub(\"aktheroy/translate_en_hi\")\n",
    "```\n",
    "- **Purpose**:\n",
    "  - Shares the fine-tuned model with the Hugging Face community by uploading it to your repository on the Hugging Face Model Hub.\n",
    "- **Repository Name**:\n",
    "  - `aktheroy/translate_en_hi`: The specified repository on the Model Hub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"aktheroy/translate_en_hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to Authenticate and Push:\n",
    "1. Log in to your Hugging Face account:\n",
    "   ```bash\n",
    "   huggingface-cli login\n",
    "   ```\n",
    "2. When running the script for the first time, you may be prompted to provide a token for authentication.\n",
    "3. Once authenticated, the model and its configuration will be uploaded to your specified repository.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages of Saving and Sharing**\n",
    "1. **Reusability**:\n",
    "   - Save the model locally for later use without retraining.\n",
    "2. **Accessibility**:\n",
    "   - Sharing the model on the Hugging Face Hub makes it accessible to other users.\n",
    "3. **Production Readiness**:\n",
    "   - Easily integrate the fine-tuned model into applications using the Hugging Face `pipeline`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Validation and Model Performance Monitoring**\n",
    "   - **Track Metrics During Training**:\n",
    "     While you've defined `metric_for_best_model=\"eval_loss\"`, it’s good practice to ensure you're tracking multiple evaluation metrics (e.g., BLEU score for translation tasks) during training to get a more comprehensive view of model performance.\n",
    "     ```python\n",
    "     from datasets import load_metric\n",
    "     bleu_metric = load_metric(\"bleu\")\n",
    "     ```\n",
    "\n",
    "   - **Evaluation During Training**:\n",
    "     Ensure that the model is evaluated on the validation set during training to check for overfitting or underfitting, and adjust hyperparameters accordingly.\n",
    "\n",
    "   - **Test Set Evaluation**:\n",
    "     After training, you should evaluate your model on a held-out test set (which you already defined) to assess its performance on unseen data. This would help gauge the generalization capability of the fine-tuned model.\n",
    "\n",
    "### **2. Hyperparameter Tuning**\n",
    "   - **Hyperparameter Search**:\n",
    "     Consider running a hyperparameter search (e.g., using `optuna` or `Ray Tune`) to optimize parameters like learning rate, batch size, and other training settings.\n",
    "\n",
    "### **3. Save and Load Checkpoints During Training**\n",
    "   - **Intermediate Checkpoints**:\n",
    "     You can save model checkpoints periodically during training to avoid losing progress in case of any interruptions.\n",
    "     ```python\n",
    "     training_args.save_steps = 500  # Saves model every 500 steps\n",
    "     ```\n",
    "\n",
    "   - **Best Model Checkpointing**:\n",
    "     Make sure you are saving the best model based on the evaluation metric.\n",
    "     ```python\n",
    "     training_args.load_best_model_at_end = True\n",
    "     ```\n",
    "\n",
    "### **4. Fine-Tuning with Data Augmentation**\n",
    "   - **Data Augmentation**:\n",
    "     For machine translation, you might consider augmenting the training data (e.g., using back-translation) to improve the model's robustness, especially if your training dataset is relatively small.\n",
    "\n",
    "### **5. Model Efficiency Improvements**\n",
    "   - **Pruning**:\n",
    "     After fine-tuning, you could explore model pruning (removing redundant neurons) to further optimize the model size and improve inference speed without sacrificing too much accuracy.\n",
    "\n",
    "### **6. Model Interpretability**\n",
    "   - **Model Explainability**:\n",
    "     Consider tools like SHAP or LIME for interpreting the model's decisions, especially when deploying the model in production or in high-stakes applications.\n",
    "\n",
    "### **7. Deployment and Inference Optimization**\n",
    "   - **ONNX Export**:\n",
    "     If you're considering deploying the model for inference on a different platform (like TensorFlow, or an edge device), you can convert the model to **ONNX** format for optimized inference.\n",
    "     ```python\n",
    "     model.save_pretrained('model_dir')\n",
    "     torch.onnx.export(model, input_tensor, 'model.onnx')\n",
    "     ```\n",
    "\n",
    "   - **Quantization for Inference**:\n",
    "     For faster inference, you could perform post-training quantization (e.g., using **8-bit quantization** with PyTorch) to reduce the model size and speed up inference on edge devices or low-resource environments.\n",
    "\n",
    "### **8. Documentation and Experiment Tracking**\n",
    "   - **Experiment Tracking**:\n",
    "     If you’re using a tool like **Weights & Biases (wandb)** or **TensorBoard**, make sure to track training and evaluation metrics for better insight into your experiments. This is useful for managing and analyzing multiple training runs.\n",
    "\n",
    "   - **Model Documentation**:\n",
    "     Document the training setup, hyperparameters, and performance metrics. This helps both for reproducibility and understanding how the model was trained when coming back to it later or sharing it with others.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Optional Steps:**\n",
    "1. Track additional evaluation metrics (e.g., BLEU score).\n",
    "2. Perform hyperparameter tuning.\n",
    "3. Save intermediate checkpoints during training.\n",
    "4. Consider data augmentation techniques (e.g., back-translation).\n",
    "5. Explore pruning for model size reduction.\n",
    "6. Use tools for model explainability.\n",
    "7. Optimize inference with ONNX or quantization.\n",
    "8. Track experiments and document the process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
