{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from time import time\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import matplotlib as pt\n",
    "\n",
    "# Third-party library imports\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from evaluate import load\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from transformers import (\n",
    "    TrainerCallback,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    "    M2M100Config,\n",
    "    M2M100ForConditionalGeneration,\n",
    "    M2M100Tokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "# Local application/library specific imports\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data loading and spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "percent_data_select = \"train[:20]\" # add percent sign ie. \"train[:20%]\" to select that percent of data \n",
    "# Load only 20% of the dataset\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"../Datasets/processed_data.csv\"}, split=percent_data_select)\n",
    "\n",
    "# Split into train and test sets (e.g., 80% train, 20% test)\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Further split the test set into validation and test (e.g., 50-50 split of the 20%)\n",
    "validation_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Combine splits into a DatasetDict\n",
    "raw_dataset = {\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"validation\": validation_test_split[\"train\"],\n",
    "    \"test\": validation_test_split[\"test\"]\n",
    "}\n",
    "\n",
    "dataset = DatasetDict(raw_dataset)\n",
    "\n",
    "# Inspect the resulting dataset\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Facebook / M2M100 418M + Lora\n",
    "- LoRA (Low-Rank Adaptation) refers to a parameter-efficient fine-tuning technique.\n",
    "\n",
    "In the context of Large Language Models (LLMs), LoRA (Low-Rank Adaptation) refers to a parameter-efficient fine-tuning technique. \n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "* **Fine-tuning:** LLMs are often pre-trained on massive datasets. Fine-tuning involves adapting these pre-trained models to specific tasks or domains using smaller, more relevant datasets.\n",
    "* **Parameter-Efficiency:** Fine-tuning LLMs can be computationally expensive, especially for very large models. LoRA addresses this by significantly reducing the number of parameters that need to be updated during fine-tuning.\n",
    "\n",
    "**How LoRA Works:**\n",
    "\n",
    "Instead of fine-tuning all the parameters of the base LLM, LoRA introduces two small, trainable matrices (A and B) for each attention layer:\n",
    "\n",
    "1. **Decomposition:** The update to the original weight matrix (W) is approximated as the product of these two smaller matrices: W' = W + A * B.\n",
    "2. **Reduced Parameters:** Since A and B have significantly fewer parameters than the original weight matrix, the overall number of trainable parameters is drastically reduced.\n",
    "3. **Fine-tuning:** Only the parameters of A and B are trained during fine-tuning, while the original weights of the base LLM remain frozen.\n",
    "\n",
    "**Benefits of LoRA:**\n",
    "\n",
    "* **Reduced Training Time and Cost:** By training only a small subset of parameters, LoRA significantly reduces training time and computational resources.\n",
    "* **Improved Efficiency:** The smaller number of parameters leads to faster inference times.\n",
    "* **Preserving Base Model:** Since the base model's weights are frozen, it retains its general knowledge and capabilities while being adapted to the specific task.\n",
    "* **Easier Deployment:** Smaller models are easier to deploy and run on devices with limited resources.\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "LoRA has been successfully applied to a wide range of LLM fine-tuning tasks, including:\n",
    "\n",
    "* **Domain Adaptation:** Adapting LLMs to specific domains like finance, medicine, or law.\n",
    "* **Task-Specific Fine-tuning:** Fine-tuning LLMs for specific tasks such as question answering, text summarization, and code generation.\n",
    "* **Personalization:** Creating personalized LLMs for individual users or groups.\n",
    "\n",
    "**In summary:**\n",
    "\n",
    "LoRA is a powerful technique that enables efficient and effective fine-tuning of LLMs. By significantly reducing the number of trainable parameters, LoRA makes it possible to customize large models for specific applications while minimizing training costs and preserving the valuable knowledge of the base model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ID = \"facebook/m2m100_418M\"  # Replace with your desired model\n",
    "model_lora = AutoModelForSeq2SeqLM.from_pretrained(model_ID)\n",
    "tokenizer_lora = AutoTokenizer.from_pretrained(model_ID, src_lang=\"en\", tgt_lang=\"hi\")\n",
    "# Create a GenerationConfig with the desired parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 text preprocessing\n",
    "\n",
    "**Importance of Text Preprocessing:**\n",
    "\n",
    "1. **Converting Text to Numbers:** Machine learning models can't directly understand raw text. Preprocessing transforms text into numerical representations (tokens) that the model can process.\n",
    "\n",
    "2. **Normalization and Consistency:** Text data can have inconsistencies like capitalization, punctuation, and variations in word forms (e.g., singular vs. plural). Preprocessing steps like lowercasing or stemming/lemmatization can address these issues, promoting consistency in the data.\n",
    "\n",
    "3. **Feature Engineering:** Preprocessing can create new features for the model. In your example, prepending \"translate Hindi to English: \" to the source sentences might help the model understand the context of translation.\n",
    "\n",
    "4. **Handling Text Length:** Different models have limitations on input and output lengths. Preprocessing techniques like truncation and padding ensure your data adheres to these limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=4,  # Low rank (fewer trainable parameters)\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\",\"k_proj\"],  # Apply LoRA to attention layers (query and value)\n",
    "    bias=\"none\",  # Specify which biases to train\n",
    "    task_type=\"SEQ_2_SEQ_LM\",  # Task type (sequence-to-sequence)\n",
    ")\n",
    "\n",
    "# Wrap the base model with LoRA\n",
    "model_lora = get_peft_model(model_lora, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_function(examples, src_lang, tgt_lang):\n",
    "    inputs = [f\"translate {src_lang} to {tgt_lang}: \" + ex for ex in examples[src_lang]]\n",
    "    targets = examples[tgt_lang]\n",
    "    model_inputs = tokenizer_lora(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer_lora.as_target_tokenizer():\n",
    "        labels = tokenizer_lora(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets_hindi_to_english = dataset.map(lambda x: preprocess_function(x, \"Hindi\", \"English\"), batched=True)\n",
    "tokenized_datasets_english_to_hindi = dataset.map(lambda x: preprocess_function(x, \"English\", \"Hindi\"), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Hindi: मंगल ग्रह के मिशन की सूची \"Launch Event Details – When did the Rovers Launch?\n",
      "Original English: \"launch event details – when did the rovers launch?\".\n",
      "Tokenized Input IDs: [128022, 5815, 80447, 11631, 128, 18006, 9, 62188, 33146, 2642, 456, 9729, 17158, 783, 79477, 33, 7939, 35233, 67444, 115309, 36, 119782, 4322, 1197, 180, 73558, 343, 35233, 24, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded Input: __en__ translate Hindi to English: मंगल ग्रह के मिशन की सूची \"Launch Event Details – When did the Rovers Launch?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Tokenized Label IDs: [128036, 33, 239, 35233, 31296, 117344, 36, 100975, 4322, 1197, 1053, 4901, 121641, 24, 860, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded Label: __hi__ \"launch event details – when did the rovers launch?\".</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "==========\n",
      "Original Hindi: \" क्या MICROSOFT ने कभी इतिहास की पुस्तकों को पढ़ा है?\n",
      "Original English: \"has microsoft ever read the history books?\".\n",
      "Tokenized Input IDs: [128022, 5815, 80447, 11631, 128, 18006, 9, 33, 10723, 100, 3549, 468, 2174, 20560, 367, 1918, 44945, 51019, 783, 95303, 23196, 929, 41165, 6045, 776, 24, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded Input: __en__ translate Hindi to English: \" क्या MICROSOFT ने कभी इतिहास की पुस्तकों को पढ़ा है?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Tokenized Label IDs: [128036, 33, 3008, 17863, 77349, 106973, 58957, 1197, 93900, 121514, 24, 860, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded Label: __hi__ \"has microsoft ever read the history books?\".</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "==========\n",
      "Original Hindi: वह उनके प्रति अत्यंत कृतज्ञ है।\n",
      "Original English: \"ədalət şükürov bio\".\n",
      "Tokenized Input IDs: [128022, 5815, 80447, 11631, 128, 18006, 9, 21510, 36030, 11209, 83112, 8233, 64801, 944, 50853, 776, 209, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded Input: __en__ translate Hindi to English: वह उनके प्रति अत्यंत कृतज्ञ है।</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Tokenized Label IDs: [128036, 33, 11580, 71, 3858, 3823, 3973, 1075, 3845, 9724, 860, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded Label: __hi__ \"ədalət şükürov bio\".</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "for idx in range(3):  # Print first 3 examples as a sample\n",
    "    print(f\"Original Hindi: {dataset['train'][idx]['Hindi']}\")\n",
    "    print(f\"Original English: {dataset['train'][idx]['English']}\")\n",
    "\n",
    "    # Tokenized inputs\n",
    "    tokenized_input = tokenized_datasets_hindi_to_english[\"train\"][idx][\"input_ids\"]\n",
    "    print(f\"Tokenized Input IDs: {tokenized_input}\")\n",
    "    print(f\"Decoded Input: {tokenizer_lora.decode(tokenized_input, skip_special_tokens=False)}\")\n",
    "\n",
    "    # Tokenized outputs\n",
    "    tokenized_label = tokenized_datasets_hindi_to_english[\"train\"][idx][\"labels\"]\n",
    "    print(f\"Tokenized Label IDs: {tokenized_label}\")\n",
    "    print(f\"Decoded Label: {tokenizer_lora.decode(tokenized_label, skip_special_tokens=False)}\")\n",
    "\n",
    "    print(\"=\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 BLEU (Bilingual Evaluation Understudy): \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer_lora, metric):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer_lora.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer_lora.pad_token_id)\n",
    "    decoded_labels = tokenizer_lora.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer_lora.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Data collator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer_lora, model=model_lora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 FineTuning\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akthe\\AppData\\Local\\Temp\\ipykernel_23460\\2401728508.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"../Model/fb/Base/Checkpoint/\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets_hindi_to_english[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_hindi_to_english[\"validation\"],\n",
    "    tokenizer=tokenizer_lora,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda x: compute_metrics(x, tokenizer_lora, metric),\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Fine-tune for English to Hindi\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets_english_to_hindi[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_english_to_hindi[\"validation\"],\n",
    "    tokenizer=tokenizer_lora,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda x: compute_metrics(x, tokenizer_lora, metric),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Saving Finetuned Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../Model/fb/LoRa/M2M100/tokenizer_config.json',\n",
       " '../Model/fb/LoRa/M2M100/special_tokens_map.json',\n",
       " '..\\\\Model\\\\fb\\\\LoRa\\\\M2M100\\\\vocab.json',\n",
       " '..\\\\Model\\\\fb\\\\LoRa\\\\M2M100\\\\sentencepiece.bpe.model',\n",
       " '../Model/fb/LoRa/M2M100/added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lora.merge_and_unload()\n",
    "model_lora.save_pretrained(\"../Model/fb/LoRa/M2M100/\")\n",
    "tokenizer_lora.save_pretrained(\"../Model/fb/LoRa/M2M100/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'This is a test.'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'यह एक परीक्षण है'}]\n"
     ]
    }
   ],
   "source": [
    "# Test Hindi to English\n",
    "translator_hindi_to_english = pipeline(\"translation_hi_to_en\", model=\"../Model/fb/LoRa/M2M100/\")\n",
    "result_hindi_to_english = translator_hindi_to_english(\"यह एक परीक्षण है\")\n",
    "print(result_hindi_to_english)\n",
    "\n",
    "# Test English to Hindi\n",
    "translator_english_to_hindi = pipeline(\"translation_en_to_hi\", model=\"../Model/fb/LoRa/M2M100/\")\n",
    "result_english_to_hindi = translator_english_to_hindi(\"This is a test\")\n",
    "print(result_english_to_hindi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WPanda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
