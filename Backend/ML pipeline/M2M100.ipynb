{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from time import time\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import matplotlib as pt\n",
    "\n",
    "# Third-party library imports\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from evaluate import load\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from transformers import (\n",
    "    TrainerCallback,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    "    M2M100Config,\n",
    "    M2M100ForConditionalGeneration,\n",
    "    M2M100Tokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "# Local application/library specific imports\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data loading and spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 35871\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 4484\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 4484\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "percent_data_select = \"train[:20%]\" # add percent sign ie. \"train[:20%]\" to select that percent of data \n",
    "# Load only 20% of the dataset\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"../Datasets/processed_data.csv\"}, split=percent_data_select)\n",
    "\n",
    "# Split into train and test sets (e.g., 80% train, 20% test)\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Further split the test set into validation and test (e.g., 50-50 split of the 20%)\n",
    "validation_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Combine splits into a DatasetDict\n",
    "raw_dataset = {\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"validation\": validation_test_split[\"train\"],\n",
    "    \"test\": validation_test_split[\"test\"]\n",
    "}\n",
    "\n",
    "dataset = DatasetDict(raw_dataset)\n",
    "\n",
    "# Inspect the resulting dataset\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'English': 'participants trained on punching bags (called a korykos).', 'Hindi': 'प्रतिभागियों को पंचिंग बैग, (जिसे कोरिकोस (Korykos) कहा जाता था, पर प्रशिक्षण दिया जाता था।'}\n",
      "{'English': 'shot over a period of twenty days, the film was primarily shot in an apartment at prabhadevi, mumbai.', 'Hindi': 'बीस दिनों की अवधि में, फिल्म को मुख्य रूप से मुंबई के प्रभादेवी क्षेत्र में स्थित एक अपार्टमेंट में शूट किया गया।'}\n",
      "{'English': 'this led the early pioneers in this type of learning to begin to apply it to \"family groups\" — that is, groups located within an organization.', 'Hindi': 'इसने इस प्रकार के प्रशिक्षण के प्रारंभिक प्रवर्तकों को इसे \"परिवार समूहों\"- अर्थात् एक ही संगठन के भीतर स्थित समूहों- पर लागू करने हेतु प्रेरित किया।'}\n",
      "{'English': 'it is the oldest in the country, having been formed in 1832, and is headquartered at 31 maitland crescent, colombo 7, close to the headquarters of sri lanka cricket.', 'Hindi': 'यह देश का सबसे पुराना देश है, जिसका गठन 1832 में हुआ था, और मुख्यालय श्रीलंका क्रिकेट के मुख्यालय के करीब 31 मैटलैंड क्रिसेंट, कोलंबो 7 में मुख्यालय है।'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])\n",
    "print(dataset['train'][1])\n",
    "print(dataset['train'][2])\n",
    "print(dataset['train'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 2. Facebook / M2M100 418M\n",
    "- M2M-100 stands for Massively Multilingual Model with 100 languages.\n",
    "- 418M refers to the model size in terms of the number of parameters. Parameters are the learnable weights within the model's neural network. A larger number of parameters generally allows the model to learn more complex patterns and achieve higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Comparing Tokenizers \n",
    "- Our approach will involve a systematic comparison of tokenization strategies, paying close attention to how each method handles linguistic characteristics specific to Hindi and English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ID = \"facebook/m2m100_418M\"  # Replace with your desired model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ID)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ID, src_lang=\"en\", tgt_lang=\"hi\")\n",
    "# Create a GenerationConfig with the desired parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Hindi Input: translate Hindi to English: पिरपाजल रेलवे सुरंग का कार्य पूरा कर लिया गया था और 2012 में परीक्षण किया जा रहा था।\n",
      "Input Tokens: ['__en__', '▁trans', 'late', '▁Hindi', '▁to', '▁English', ':', '▁प', 'िर', 'पा', 'जल', '▁रेल', 'वे', '▁सुर', 'ंग', '▁का', '▁कार्य', '▁पूरा', '▁कर', '▁लिया', '▁गया', '▁था', '▁और', '▁2012', '▁में', '▁परी', 'क्षण', '▁किया', '▁जा', '▁रहा', '▁था', '।', '</s>']\n",
      "Decoded Input Text: __en__ translate Hindi to English: पिरपाजल रेलवे सुरंग का कार्य पूरा कर लिया गया था और 2012 में परीक्षण किया जा रहा था।</s>\n",
      "\n",
      "Original English Target: pirpanjal railway tunnel was completed and testing was being conducted in 2012.\n",
      "Target Tokens: ['__hi__', '▁pir', 'pan', 'jal', '▁ra', 'il', 'way', '▁tunnel', '▁was', '▁complet', 'ed', '▁and', '▁testing', '▁was', '▁being', '▁condu', 'cted', '▁in', '▁2012.', '</s>']\n",
      "Decoded Target Text: __hi__ pirpanjal railway tunnel was completed and testing was being conducted in 2012.</s>\n"
     ]
    }
   ],
   "source": [
    "# Sample data from the dataset\n",
    "sample = dataset[\"train\"][12]  # Replace with an actual sample index\n",
    "input_text = \"translate Hindi to English: \" + sample[\"Hindi\"]\n",
    "target_text = sample[\"English\"]\n",
    "\n",
    "# Tokenize inputs and targets\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(target_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Convert token IDs back to tokens\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "target_tokens = tokenizer.convert_ids_to_tokens(targets[\"input_ids\"][0])\n",
    "\n",
    "# Print the results for inspection\n",
    "print(\"Original Hindi Input:\", input_text)\n",
    "print(\"Input Tokens:\", input_tokens)\n",
    "print(\"Decoded Input Text:\", tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=False))\n",
    "print(\"\\nOriginal English Target:\", target_text)\n",
    "print(\"Target Tokens:\", target_tokens)\n",
    "print(\"Decoded Target Text:\", tokenizer.decode(targets[\"input_ids\"][0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Why are \"__hi__\" and \"__en__\" appearing before the English and Hindi sentences, respectively?**\n",
    "\n",
    "These special tokens, \"__hi__\" and \"__en__\", are often used in machine translation tasks to explicitly indicate the **source language** and **target language**. They serve as markers for the model to understand which language it's currently processing.\n",
    "\n",
    "##### Here's a breakdown of their functions:\n",
    "\n",
    "* **__hi__ and __en__: Language Identifiers:**\n",
    "   - **__hi__**: This token typically represents the Hindi language. When placed at the beginning of a sentence, it tells the model that the following text is in Hindi and should be translated into the target language (in this case, English).\n",
    "   - **__en__**: Similarly, \"__en__\" indicates that the following text is in English. It might be used in scenarios where the model is asked to translate from English to Hindi or for tasks like language identification.\n",
    "\n",
    "##### Why are they added?\n",
    "\n",
    "* **Clarity for the Model:** These markers provide clear and explicit information to the model about the language of the input and output sequences. This helps the model to better understand the context and improve the accuracy of its translations.\n",
    "* **Handling Multiple Language Pairs:** In multilingual models, these tokens can be used to handle various language pairs. For example, if the model is trained on multiple language pairs (e.g., English-French, English-Spanish), these tokens can help the model distinguish between the different language pairs.\n",
    "* **Facilitating Language Identification:** In some cases, these tokens can also be used for language identification tasks, where the model is asked to determine the language of a given text.\n",
    "\n",
    "##### In above given example:\n",
    "\n",
    "* **\"__en__ translate Hindi to English: जॉर्ज मिलर हमेशा एक व्यक्ति को दोनों करना चाहते थे।\"**: This part tells the model that the following text is in English and the task is to translate the Hindi text that follows.\n",
    "* **\"__hi__ george miller always wanted one person to do both .\"**: This indicates that the following text is the English translation of the preceding Hindi text.\n",
    "\n",
    "**Note:** The specific tokens used (e.g., \"__hi__\", \"__en__\") can vary depending on the pre-training data and the specific configuration of the model. However, the general concept of using special tokens to indicate language remains consistent.\n",
    "\n",
    "By understanding the role of these tokens, one can better interpret the model's output and fine-tune your training data for more accurate translations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 M2M100 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ID_fb = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\",)\n",
    "tokenizer_fb = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\", src_lang=\"en\", tgt_lang=\"hi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Hindi Input: translate Hindi to English: पिरपाजल रेलवे सुरंग का कार्य पूरा कर लिया गया था और 2012 में परीक्षण किया जा रहा था।\n",
      "Input Tokens: ['__en__', '▁trans', 'late', '▁Hindi', '▁to', '▁English', ':', '▁प', 'िर', 'पा', 'जल', '▁रेल', 'वे', '▁सुर', 'ंग', '▁का', '▁कार्य', '▁पूरा', '▁कर', '▁लिया', '▁गया', '▁था', '▁और', '▁2012', '▁में', '▁परी', 'क्षण', '▁किया', '▁जा', '▁रहा', '▁था', '।', '</s>']\n",
      "Decoded Input Text: __en__ translate Hindi to English: पिरपाजल रेलवे सुरंग का कार्य पूरा कर लिया गया था और 2012 में परीक्षण किया जा रहा था।</s>\n",
      "\n",
      "Original English Target: pirpanjal railway tunnel was completed and testing was being conducted in 2012.\n",
      "Target Tokens: ['__hi__', '▁pir', 'pan', 'jal', '▁ra', 'il', 'way', '▁tunnel', '▁was', '▁complet', 'ed', '▁and', '▁testing', '▁was', '▁being', '▁condu', 'cted', '▁in', '▁2012.', '</s>']\n",
      "Decoded Target Text: __hi__ pirpanjal railway tunnel was completed and testing was being conducted in 2012.</s>\n"
     ]
    }
   ],
   "source": [
    "# Sample data from the dataset\n",
    "sample2 = dataset[\"train\"][12]  # Replace with an actual sample index\n",
    "input_text2 = \"translate Hindi to English: \" + sample2[\"Hindi\"]\n",
    "target_text2 = sample2[\"English\"]\n",
    "\n",
    "# Tokenize inputs and targets\n",
    "inputs2 = tokenizer_fb(input_text2, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "with tokenizer_fb.as_target_tokenizer():\n",
    "    targets2 = tokenizer_fb(target_text2, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Convert token IDs back to tokens\n",
    "input_tokens2 = tokenizer_fb.convert_ids_to_tokens(inputs2[\"input_ids\"][0])\n",
    "target_tokens2 = tokenizer_fb.convert_ids_to_tokens(targets2[\"input_ids\"][0])\n",
    "\n",
    "# Print the results for inspection\n",
    "print(\"Original Hindi Input:\", input_text2)\n",
    "print(\"Input Tokens:\", input_tokens2)\n",
    "print(\"Decoded Input Text:\", tokenizer_fb.decode(inputs2[\"input_ids\"][0], skip_special_tokens=False))\n",
    "print(\"\\nOriginal English Target:\", target_text2)\n",
    "print(\"Target Tokens:\", target_tokens2)\n",
    "print(\"Decoded Target Text:\", tokenizer_fb.decode(targets2[\"input_ids\"][0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'additional_special_tokens': ['__af__', '__am__', '__ar__', '__ast__', '__az__', '__ba__', '__be__', '__bg__', '__bn__', '__br__', '__bs__', '__ca__', '__ceb__', '__cs__', '__cy__', '__da__', '__de__', '__el__', '__en__', '__es__', '__et__', '__fa__', '__ff__', '__fi__', '__fr__', '__fy__', '__ga__', '__gd__', '__gl__', '__gu__', '__ha__', '__he__', '__hi__', '__hr__', '__ht__', '__hu__', '__hy__', '__id__', '__ig__', '__ilo__', '__is__', '__it__', '__ja__', '__jv__', '__ka__', '__kk__', '__km__', '__kn__', '__ko__', '__lb__', '__lg__', '__ln__', '__lo__', '__lt__', '__lv__', '__mg__', '__mk__', '__ml__', '__mn__', '__mr__', '__ms__', '__my__', '__ne__', '__nl__', '__no__', '__ns__', '__oc__', '__or__', '__pa__', '__pl__', '__ps__', '__pt__', '__ro__', '__ru__', '__sd__', '__si__', '__sk__', '__sl__', '__so__', '__sq__', '__sr__', '__ss__', '__su__', '__sv__', '__sw__', '__ta__', '__th__', '__tl__', '__tn__', '__tr__', '__uk__', '__ur__', '__uz__', '__vi__', '__wo__', '__xh__', '__yi__', '__yo__', '__zh__', '__zu__']}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.special_tokens_map)  # Check special tokens (e.g., <pad>, <unk>, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'additional_special_tokens': ['__af__', '__am__', '__ar__', '__ast__', '__az__', '__ba__', '__be__', '__bg__', '__bn__', '__br__', '__bs__', '__ca__', '__ceb__', '__cs__', '__cy__', '__da__', '__de__', '__el__', '__en__', '__es__', '__et__', '__fa__', '__ff__', '__fi__', '__fr__', '__fy__', '__ga__', '__gd__', '__gl__', '__gu__', '__ha__', '__he__', '__hi__', '__hr__', '__ht__', '__hu__', '__hy__', '__id__', '__ig__', '__ilo__', '__is__', '__it__', '__ja__', '__jv__', '__ka__', '__kk__', '__km__', '__kn__', '__ko__', '__lb__', '__lg__', '__ln__', '__lo__', '__lt__', '__lv__', '__mg__', '__mk__', '__ml__', '__mn__', '__mr__', '__ms__', '__my__', '__ne__', '__nl__', '__no__', '__ns__', '__oc__', '__or__', '__pa__', '__pl__', '__ps__', '__pt__', '__ro__', '__ru__', '__sd__', '__si__', '__sk__', '__sl__', '__so__', '__sq__', '__sr__', '__ss__', '__su__', '__sv__', '__sw__', '__ta__', '__th__', '__tl__', '__tn__', '__tr__', '__uk__', '__ur__', '__uz__', '__vi__', '__wo__', '__xh__', '__yi__', '__yo__', '__zh__', '__zu__']}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_fb.special_tokens_map)  # Check special tokens (e.g., <pad>, <unk>, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Both tokenizers i.e. pretrained models and model-specific tokenizers can yield similar outputs in many scenarios, but this isn't a universal rule.**\n",
    "\n",
    "  -  ***When comparing different models, such as `M2M100` and `Google's T5 small`, we'll encounter variations in how unknown words are processed. The Google T5 small model, for instance, tends to generate more `<unk>` tokens when encountering vocabulary outside its training set, which can pose challenges for translation*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 text preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Importance of Text Preprocessing:**\n",
    "\n",
    "1. **Converting Text to Numbers:** Machine learning models can't directly understand raw text. Preprocessing transforms text into numerical representations (tokens) that the model can process.\n",
    "\n",
    "2. **Normalization and Consistency:** Text data can have inconsistencies like capitalization, punctuation, and variations in word forms (e.g., singular vs. plural). Preprocessing steps like lowercasing or stemming/lemmatization can address these issues, promoting consistency in the data.\n",
    "\n",
    "3. **Feature Engineering:** Preprocessing can create new features for the model. In your example, prepending \"translate Hindi to English: \" to the source sentences might help the model understand the context of translation.\n",
    "\n",
    "4. **Handling Text Length:** Different models have limitations on input and output lengths. Preprocessing techniques like truncation and padding ensure your data adheres to these limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_function(examples, src_lang, tgt_lang):\n",
    "    inputs = [f\"translate {src_lang} to {tgt_lang}: \" + ex for ex in examples[src_lang]]\n",
    "    targets = examples[tgt_lang]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39dae838423f48a49f47f3ccdfb9f926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35871 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e178e1cb7d6a47dfb5023d85805f0e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "894b5de13a0148b185b3c2b0fb77099d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e9928d78d84a978a60044524439417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35871 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37fccc6e6eb4d6f9ab2bbddc506829c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c0bb52563745538c34374057233e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets_hindi_to_english = dataset.map(lambda x: preprocess_function(x, \"Hindi\", \"English\"), batched=True)\n",
    "tokenized_datasets_english_to_hindi = dataset.map(lambda x: preprocess_function(x, \"English\", \"Hindi\"), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Hindi: प्रतिभागियों को पंचिंग बैग, (जिसे कोरिकोस (Korykos) कहा जाता था, पर प्रशिक्षण दिया जाता था।\n",
      "Original English: participants trained on punching bags (called a korykos).\n",
      "Tokenized Input IDs: [128022, 5815, 80447, 11631, 128, 18006, 9, 11209, 46644, 10531, 929, 2846, 14095, 52053, 36608, 2568, 4, 11, 16071, 5398, 929, 19404, 530, 961, 11, 358, 178, 1844, 63, 17, 11259, 27741, 6904, 4, 1664, 82554, 23441, 15795, 27741, 6904, 209, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded Input: __en__ translate Hindi to English: प्रतिभागियों को पंचिंग बैग, (जिसे कोरिकोस (Korykos) कहा जाता था, पर प्रशिक्षण दिया जाता था।</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Tokenized Label IDs: [128036, 87587, 1368, 97863, 149, 2560, 117259, 98329, 11, 143, 916, 241, 8, 1986, 1844, 63, 188, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded Label: __hi__ participants trained on punching bags (called a korykos).</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "==========\n",
      "Original Hindi: बीस दिनों की अवधि में, फिल्म को मुख्य रूप से मुंबई के प्रभादेवी क्षेत्र में स्थित एक अपार्टमेंट में शूट किया गया।\n",
      "Original English: shot over a period of twenty days, the film was primarily shot in an apartment at prabhadevi, mumbai.\n",
      "Tokenized Input IDs: [128022, 5815, 80447, 11631, 128, 18006, 9, 20682, 961, 87753, 783, 17378, 16814, 698, 4, 20046, 929, 20393, 25753, 1044, 25765, 456, 2860, 13140, 15865, 5509, 16013, 698, 74863, 1618, 17012, 5927, 25886, 67318, 698, 3776, 47094, 8053, 10397, 209, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded Input: __en__ translate Hindi to English: बीस दिनों की अवधि में, फिल्म को मुख्य रूप से मुंबई के प्रभादेवी क्षेत्र में स्थित एक अपार्टमेंट में शूट किया गया।</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Tokenized Label IDs: [128036, 119398, 1120, 8, 17569, 432, 31993, 60853, 70398, 4, 1197, 2140, 1513, 116275, 10287, 119398, 28, 48, 122333, 120, 2688, 4843, 692, 737, 4, 172, 4766, 293, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded Label: __hi__ shot over a period of twenty days, the film was primarily shot in an apartment at prabhadevi, mumbai.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "==========\n",
      "Original Hindi: इसने इस प्रकार के प्रशिक्षण के प्रारंभिक प्रवर्तकों को इसे \"परिवार समूहों\"- अर्थात् एक ही संगठन के भीतर स्थित समूहों- पर लागू करने हेतु प्रेरित किया।\n",
      "Original English: this led the early pioneers in this type of learning to begin to apply it to \"family groups\" — that is, groups located within an organization.\n",
      "Tokenized Input IDs: [128022, 5815, 80447, 11631, 128, 18006, 9, 5163, 983, 5163, 29414, 456, 82554, 23441, 456, 108100, 101381, 2877, 55506, 88209, 23196, 929, 52374, 33, 25238, 17710, 5927, 56420, 1839, 30808, 79820, 11432, 1618, 4105, 81094, 456, 3211, 18561, 74863, 56420, 1839, 7, 1664, 67250, 7988, 6174, 30335, 114419, 2903, 8053, 209, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded Input: __en__ translate Hindi to English: इसने इस प्रकार के प्रशिक्षण के प्रारंभिक प्रवर्तकों को इसे \"परिवार समूहों\"- अर्थात् एक ही संगठन के भीतर स्थित समूहों- पर लागू करने हेतु प्रेरित किया।</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Tokenized Label IDs: [128036, 15911, 7587, 1197, 123665, 266, 2840, 397, 28, 15911, 24434, 432, 120932, 128, 24952, 128, 123086, 862, 128, 33, 34187, 10287, 125115, 64, 355, 16076, 117, 4, 125115, 125560, 123502, 48, 125521, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded Label: __hi__ this led the early pioneers in this type of learning to begin to apply it to \"family groups\" — that is, groups located within an organization.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first example\n",
    "for idx in range(3):  # Print first 3 examples as a sample\n",
    "    print(f\"Original Hindi: {dataset['train'][idx]['Hindi']}\")\n",
    "    print(f\"Original English: {dataset['train'][idx]['English']}\")\n",
    "\n",
    "    # Tokenized inputs\n",
    "    tokenized_input = tokenized_datasets_hindi_to_english[\"train\"][idx][\"input_ids\"]\n",
    "    print(f\"Tokenized Input IDs: {tokenized_input}\")\n",
    "    print(f\"Decoded Input: {tokenizer.decode(tokenized_input, skip_special_tokens=False)}\")\n",
    "\n",
    "    # Tokenized outputs\n",
    "    tokenized_label = tokenized_datasets_hindi_to_english[\"train\"][idx][\"labels\"]\n",
    "    print(f\"Tokenized Label IDs: {tokenized_label}\")\n",
    "    print(f\"Decoded Label: {tokenizer.decode(tokenized_label, skip_special_tokens=False)}\")\n",
    "\n",
    "    print(\"=\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specific Breakdown to Code:**\n",
    "\n",
    "- **`inputs = [\"translate Hindi to English: \" + ex for ex in examples[\"Hindi\"]]`**: This line creates a new list (`inputs`) by prepending a context string to each sentence in the `Hindi` column of the dataset.\n",
    "\n",
    "- **`model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")`**: This tokenizes the `inputs` list, converting them into numerical representations using the `tokenizer`. The `max_length` argument limits the length of each sequence, and `truncation=True` ensures sequences exceeding the limit are shortened. `padding=\"max_length\"` pads shorter sequences with special tokens to create a uniform length.\n",
    "\n",
    "- **`with tokenizer.as_target_tokenizer():`**: This context manager configures the tokenizer for handling the target language (English) by setting the appropriate attributes.\n",
    "\n",
    "- **`labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")`**: This tokenizes the target sentences (`English`) with similar parameters as the input.\n",
    "\n",
    "- **`model_inputs[\"labels\"] = labels[\"input_ids\"]`**: This adds the tokenized target sentence IDs (stored as `input_ids` in the `labels` dictionary) as a new key \"labels\" within the `model_inputs` dictionary.\n",
    "\n",
    "- **`tokenized_datasets = dataset.map(preprocess_function, batched=True)`**: This line applies the `preprocess_function` to each element of the dataset (`dataset`) in batches using `batched=True` for efficiency. The resulting processed data is stored in `tokenized_datasets`.\n",
    "\n",
    "In summary, this text preprocessing step transforms your raw text data into a format suitable for training your machine translation model. It ensures consistency, handles sequence lengths, and potentially adds contextual information to aid the translation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Data collator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, particularly for transformer models, a data collator plays a crucial role in preparing batches of data for training. It's essentially a function that takes individual samples and combines them into batches in a way that's efficient and optimized for the model's processing.\n",
    "\n",
    "\n",
    "For sequence-to-sequence tasks like translation, a specialized data collator (often DataCollatorForSeq2Seq) becomes critical. It ensures that:\n",
    "- Source and target sequences are properly aligned\n",
    "- Padding is applied consistently\n",
    "- Attention mechanisms can correctly ignore padded tokens\n",
    "- Labels are prepared in a format that allows for loss calculation during training\n",
    "\n",
    "\n",
    "\n",
    "Without a proper data collator, you'd need to manually handle sequence padding, masking, and batch preparation, which would be computationally expensive and error-prone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 BLEU (Bilingual Evaluation Understudy): \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer, metric):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Fine Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akthe\\AppData\\Local\\Temp\\ipykernel_5952\\902773401.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a364153afb46d2984cdd8c41aed8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7837, 'grad_norm': 1.3042411804199219, 'learning_rate': 1.9262563187630093e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akthe\\miniconda3\\envs\\WPanda\\Lib\\site-packages\\transformers\\modeling_utils.py:2748: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"../Model/fb/Base/Checkpoint/\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets_hindi_to_english[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_hindi_to_english[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda x: compute_metrics(x, tokenizer, metric),\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Fine-tune for English to Hindi\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets_english_to_hindi[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_english_to_hindi[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda x: compute_metrics(x, tokenizer, metric),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Saving Finetuned Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../Model/fb/Base/M2M100/tokenizer_config.json',\n",
       " '../Model/fb/Base/M2M100/special_tokens_map.json',\n",
       " '..\\\\Model\\\\fb\\\\Base\\\\M2M100\\\\vocab.json',\n",
       " '..\\\\Model\\\\fb\\\\Base\\\\M2M100\\\\sentencepiece.bpe.model',\n",
       " '../Model/fb/Base/M2M100/added_tokens.json')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"../Model/fb/Base/M2M100/\")\n",
    "tokenizer.save_pretrained(\"../Model/fb/Base/M2M100/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 laoding & testing Finetuned Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'एक पैर तोड़ना'}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "text = 'break a leg'\n",
    "translator = pipeline(\"translation_en_to_hi\", model=\"../Model/fb/Base/M2M100/\")\n",
    "translator(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'It is a test.'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'यह एक परीक्षण है।'}]\n"
     ]
    }
   ],
   "source": [
    "# Test Hindi to English\n",
    "translator_hindi_to_english = pipeline(\"translation_hi_to_en\", model=\"../Model/fb/Base/M2M100/\")\n",
    "result_hindi_to_english = translator_hindi_to_english(\"यह एक परीक्षण है\")\n",
    "print(result_hindi_to_english)\n",
    "\n",
    "# Test English to Hindi\n",
    "translator_english_to_hindi = pipeline(\"translation_en_to_hi\", model=\"../Model/fb/Base/M2M100/\")\n",
    "result_english_to_hindi = translator_english_to_hindi(\"This is a test\")\n",
    "print(result_english_to_hindi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.\tFacebook / M2M100 418M + LoRa\n",
    "- LoRA (Low-Rank Adaptation) refers to a parameter-efficient fine-tuning technique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of Large Language Models (LLMs), LoRA (Low-Rank Adaptation) refers to a parameter-efficient fine-tuning technique. \n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "* **Fine-tuning:** LLMs are often pre-trained on massive datasets. Fine-tuning involves adapting these pre-trained models to specific tasks or domains using smaller, more relevant datasets.\n",
    "* **Parameter-Efficiency:** Fine-tuning LLMs can be computationally expensive, especially for very large models. LoRA addresses this by significantly reducing the number of parameters that need to be updated during fine-tuning.\n",
    "\n",
    "**How LoRA Works:**\n",
    "\n",
    "Instead of fine-tuning all the parameters of the base LLM, LoRA introduces two small, trainable matrices (A and B) for each attention layer:\n",
    "\n",
    "1. **Decomposition:** The update to the original weight matrix (W) is approximated as the product of these two smaller matrices: W' = W + A * B.\n",
    "2. **Reduced Parameters:** Since A and B have significantly fewer parameters than the original weight matrix, the overall number of trainable parameters is drastically reduced.\n",
    "3. **Fine-tuning:** Only the parameters of A and B are trained during fine-tuning, while the original weights of the base LLM remain frozen.\n",
    "\n",
    "**Benefits of LoRA:**\n",
    "\n",
    "* **Reduced Training Time and Cost:** By training only a small subset of parameters, LoRA significantly reduces training time and computational resources.\n",
    "* **Improved Efficiency:** The smaller number of parameters leads to faster inference times.\n",
    "* **Preserving Base Model:** Since the base model's weights are frozen, it retains its general knowledge and capabilities while being adapted to the specific task.\n",
    "* **Easier Deployment:** Smaller models are easier to deploy and run on devices with limited resources.\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "LoRA has been successfully applied to a wide range of LLM fine-tuning tasks, including:\n",
    "\n",
    "* **Domain Adaptation:** Adapting LLMs to specific domains like finance, medicine, or law.\n",
    "* **Task-Specific Fine-tuning:** Fine-tuning LLMs for specific tasks such as question answering, text summarization, and code generation.\n",
    "* **Personalization:** Creating personalized LLMs for individual users or groups.\n",
    "\n",
    "**In summary:**\n",
    "\n",
    "LoRA is a powerful technique that enables efficient and effective fine-tuning of LLMs. By significantly reducing the number of trainable parameters, LoRA makes it possible to customize large models for specific applications while minimizing training costs and preserving the valuable knowledge of the base model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Model self attention layer\n",
    "- it is crucial to know self attention module defined as in lora weight changes of these metrics are tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " M2M100ForConditionalGeneration(\n",
      "  (model): M2M100Model(\n",
      "    (shared): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "    (encoder): M2M100Encoder(\n",
      "      (embed_tokens): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x M2M100EncoderLayer(\n",
      "          (self_attn): M2M100SdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): ReLU()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): M2M100Decoder(\n",
      "      (embed_tokens): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x M2M100DecoderLayer(\n",
      "          (self_attn): M2M100SdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): M2M100SdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=128112, bias=False)\n",
      ")\n",
      "model M2M100Model(\n",
      "  (shared): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "  (encoder): M2M100Encoder(\n",
      "    (embed_tokens): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "    (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x M2M100EncoderLayer(\n",
      "        (self_attn): M2M100SdpaAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): ReLU()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): M2M100Decoder(\n",
      "    (embed_tokens): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "    (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x M2M100DecoderLayer(\n",
      "        (self_attn): M2M100SdpaAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): M2M100SdpaAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "model.shared M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "model.encoder M2M100Encoder(\n",
      "  (embed_tokens): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "  (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
      "  (layers): ModuleList(\n",
      "    (0-11): 12 x M2M100EncoderLayer(\n",
      "      (self_attn): M2M100SdpaAttention(\n",
      "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (activation_fn): ReLU()\n",
      "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.embed_tokens M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "model.encoder.embed_positions M2M100SinusoidalPositionalEmbedding()\n",
      "model.encoder.layers ModuleList(\n",
      "  (0-11): 12 x M2M100EncoderLayer(\n",
      "    (self_attn): M2M100SdpaAttention(\n",
      "      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (activation_fn): ReLU()\n",
      "    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "model.encoder.layers.0 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.0.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.0.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.0.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.0.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.0.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.0.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.0.activation_fn ReLU()\n",
      "model.encoder.layers.0.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.0.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.0.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.1 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.1.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.1.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.1.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.1.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.1.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.1.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.1.activation_fn ReLU()\n",
      "model.encoder.layers.1.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.1.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.1.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.2 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.2.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.2.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.2.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.2.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.2.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.2.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.2.activation_fn ReLU()\n",
      "model.encoder.layers.2.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.2.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.2.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.3 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.3.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.3.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.3.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.3.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.3.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.3.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.3.activation_fn ReLU()\n",
      "model.encoder.layers.3.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.3.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.3.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.4 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.4.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.4.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.4.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.4.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.4.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.4.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.4.activation_fn ReLU()\n",
      "model.encoder.layers.4.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.4.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.4.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.5 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.5.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.5.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.5.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.5.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.5.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.5.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.5.activation_fn ReLU()\n",
      "model.encoder.layers.5.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.5.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.5.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.6 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.6.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.6.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.6.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.6.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.6.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.6.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.6.activation_fn ReLU()\n",
      "model.encoder.layers.6.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.6.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.6.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.7 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.7.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.7.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.7.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.7.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.7.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.7.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.7.activation_fn ReLU()\n",
      "model.encoder.layers.7.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.7.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.7.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.8 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.8.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.8.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.8.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.8.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.8.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.8.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.8.activation_fn ReLU()\n",
      "model.encoder.layers.8.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.8.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.8.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.9 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.9.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.9.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.9.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.9.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.9.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.9.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.9.activation_fn ReLU()\n",
      "model.encoder.layers.9.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.9.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.9.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.10 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.10.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.10.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.10.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.10.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.10.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.10.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.10.activation_fn ReLU()\n",
      "model.encoder.layers.10.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.10.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.10.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.11 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.11.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.11.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.11.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.11.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.11.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.11.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.11.activation_fn ReLU()\n",
      "model.encoder.layers.11.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.11.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.11.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder M2M100Decoder(\n",
      "  (embed_tokens): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "  (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
      "  (layers): ModuleList(\n",
      "    (0-11): 12 x M2M100DecoderLayer(\n",
      "      (self_attn): M2M100SdpaAttention(\n",
      "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder_attn): M2M100SdpaAttention(\n",
      "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.embed_tokens M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "model.decoder.embed_positions M2M100SinusoidalPositionalEmbedding()\n",
      "model.decoder.layers ModuleList(\n",
      "  (0-11): 12 x M2M100DecoderLayer(\n",
      "    (self_attn): M2M100SdpaAttention(\n",
      "      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder_attn): M2M100SdpaAttention(\n",
      "      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "model.decoder.layers.0 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.0.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.0.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.activation_fn ReLU()\n",
      "model.decoder.layers.0.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.0.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.0.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.0.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.0.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.1 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.1.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.1.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.activation_fn ReLU()\n",
      "model.decoder.layers.1.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.1.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.1.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.1.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.1.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.2 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.2.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.2.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.activation_fn ReLU()\n",
      "model.decoder.layers.2.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.2.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.2.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.2.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.2.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.3 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.3.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.3.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.activation_fn ReLU()\n",
      "model.decoder.layers.3.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.3.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.3.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.3.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.3.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.4 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.4.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.4.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.activation_fn ReLU()\n",
      "model.decoder.layers.4.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.4.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.4.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.4.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.4.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.5 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.5.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.5.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.activation_fn ReLU()\n",
      "model.decoder.layers.5.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.5.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.5.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.5.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.5.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.6 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.6.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.6.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.activation_fn ReLU()\n",
      "model.decoder.layers.6.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.6.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.6.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.6.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.6.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.7 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.7.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.7.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.activation_fn ReLU()\n",
      "model.decoder.layers.7.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.7.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.7.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.7.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.7.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.8 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.8.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.8.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.activation_fn ReLU()\n",
      "model.decoder.layers.8.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.8.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.8.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.8.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.8.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.9 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.9.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.9.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.activation_fn ReLU()\n",
      "model.decoder.layers.9.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.9.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.9.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.9.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.9.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.10 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.10.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.10.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.activation_fn ReLU()\n",
      "model.decoder.layers.10.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.10.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.10.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.10.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.10.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.11 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.11.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.11.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.activation_fn ReLU()\n",
      "model.decoder.layers.11.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.11.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.11.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.11.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.11.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "lm_head Linear(in_features=1024, out_features=128112, bias=False)\n"
     ]
    }
   ],
   "source": [
    "# Access the model's named modules\n",
    "for name, module in model.named_modules():\n",
    "    print(name, module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Defining lora Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ID = \"facebook/m2m100_418M\"  # Replace with your desired model\n",
    "model_lora = AutoModelForSeq2SeqLM.from_pretrained(model_ID)\n",
    "tokenizer_lora = AutoTokenizer.from_pretrained(model_ID, src_lang=\"en\", tgt_lang=\"hi\")\n",
    "# Create a GenerationConfig with the desired parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=4,  # Low rank (fewer trainable parameters)\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Apply LoRA to attention layers (query and value)\n",
    "    bias=\"none\",  # Specify which biases to train\n",
    "    task_type=\"SEQ_2_SEQ_LM\",  # Task type (sequence-to-sequence)\n",
    ")\n",
    "\n",
    "# Wrap the base model with LoRA\n",
    "model_lora = get_peft_model(model_lora, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 loading and spliting datasets for lora \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 17935\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 2242\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 2242\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "percent_data_select_lora = \"train[:10%]\" # add percent sign ie. \"train[:20%]\" to select that percent of data \n",
    "# Load only 20% of the dataset\n",
    "dataset_lora = load_dataset(\"csv\", data_files={\"train\": \"../Datasets/processed_data.csv\"}, split=percent_data_select_lora)\n",
    "\n",
    "# Split into train and test sets (e.g., 80% train, 20% test)\n",
    "train_test_split = dataset_lora.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Further split the test set into validation and test (e.g., 50-50 split of the 20%)\n",
    "validation_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Combine splits into a DatasetDict\n",
    "raw_dataset = {\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"validation\": validation_test_split[\"train\"],\n",
    "    \"test\": validation_test_split[\"test\"]\n",
    "}\n",
    "\n",
    "dataset_lora = DatasetDict(raw_dataset)\n",
    "\n",
    "# Inspect the resulting dataset\n",
    "print(dataset_lora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Tokenising dataset for lora \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4521d5cab8c1443e9379ea218199e7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17935 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akthe\\miniconda3\\envs\\WPanda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3957: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64aeb4e3b46e4befb68a026afb030424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2242 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b283a7e4b2bd4238a023712a7e975a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2242 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [\"translate Hindi to English: \" + ex for ex in examples[\"Hindi\"]]\n",
    "    targets = examples[\"English\"]\n",
    "    model_inputs = tokenizer_lora(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer_lora.as_target_tokenizer():\n",
    "        labels = tokenizer_lora(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_dataset_lora = dataset_lora.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['English', 'Hindi', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 17935\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['English', 'Hindi', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2242\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['English', 'Hindi', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2242\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer_lora, model=model_lora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 FineTuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 23\u001b[0m\n\u001b[0;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[0;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Model/fb/LoRa/Checkpoint/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     eval_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m     17\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_lora,\n\u001b[0;32m     18\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     19\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset_lora[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     20\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset_lora[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     21\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer_lora,\n\u001b[0;32m     22\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m---> 23\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[43mcompute_metrics\u001b[49m,\n\u001b[0;32m     24\u001b[0m     \n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'compute_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"../Model/fb/LoRa/Checkpoint/\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    save_safetensors=False,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True, #change to bf16=True for XPU\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_lora[\"train\"],\n",
    "    eval_dataset=tokenized_dataset_lora[\"test\"],\n",
    "    tokenizer=tokenizer_lora,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Saving Finetuned Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../Model/fb/LoRa/M2M100/tokenizer_config.json',\n",
       " '../Model/fb/LoRa/M2M100/special_tokens_map.json',\n",
       " '..\\\\Model\\\\fb\\\\LoRa\\\\M2M100\\\\vocab.json',\n",
       " '..\\\\Model\\\\fb\\\\LoRa\\\\M2M100\\\\sentencepiece.bpe.model',\n",
       " '../Model/fb/LoRa/M2M100/added_tokens.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lora.merge_and_unload()\n",
    "model_lora.save_pretrained(\"../Model/fb/LoRa/M2M100/\")\n",
    "tokenizer.save_pretrained(\"../Model/fb/LoRa/M2M100/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Testing Finetuned Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'एक पैर तोड़ना'}]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the translation pipeline\n",
    "translator = pipeline(\"translation_en_to_hi\", model=\"../Model/fb/LoRa/M2M100/\", tokenizer=\"../Model/fb/LoRa/M2M100/\")\n",
    "\n",
    "# Test translation\n",
    "text = \"break a leg\"\n",
    "translated_text = translator(text)\n",
    "\n",
    "print(translated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Comparision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- **BLEU Score Comparison**:\n",
    "  - A bar chart showing the BLEU scores for the untrained model, fine-tuned model, and LoRA model.\n",
    "  - You should see that the fine-tuned model and LoRA model perform better than the untrained model.\n",
    "\n",
    "- **Loss Comparison**:\n",
    "  - A bar chart showing the loss for the untrained model, fine-tuned model, and LoRA model.\n",
    "  - The fine-tuned model and LoRA model should have lower loss compared to the untrained model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akthe\\miniconda3\\envs\\WPanda\\Lib\\site-packages\\peft\\peft_model.py:609: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.0.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.0.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.0.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.0.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.1.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.1.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.1.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.1.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.2.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.2.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.2.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.2.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.3.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.3.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.3.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.3.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.4.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.4.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.4.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.4.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.5.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.5.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.5.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.5.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.6.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.6.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.6.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.6.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.7.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.7.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.7.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.7.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.8.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.8.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.8.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.8.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.9.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.9.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.9.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.9.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.10.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.10.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.10.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.10.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.11.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.11.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.11.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.11.encoder_attn.q_proj.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n",
      "C:\\Users\\akthe\\AppData\\Local\\Temp\\ipykernel_20844\\1769570818.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431553fa6c62474cbb40037ea74da27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb3195840964ca88b1d5784d3e9fc95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30593ebf89b4495a4a151a714cab767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Load the datasets\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "# Define a function to evaluate a model\n",
    "def evaluate_model(model, test_dataset):\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=Seq2SeqTrainingArguments(output_dir=\"../Model/temp\", per_device_eval_batch_size=8, predict_with_generate=True),\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    results = trainer.evaluate()\n",
    "    return results[\"eval_bleu\"], results[\"eval_loss\"]\n",
    "\n",
    "# Load the untrained model\n",
    "untrained_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Load the fine-tuned model\n",
    "fine_tuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"../Model/fb/Base/M2M100/\")\n",
    "\n",
    "# Load the fine-tuned LoRA model\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    AutoModelForSeq2SeqLM.from_pretrained(\"facebook/m2m100_418M\"),\n",
    "    \"../Model/fb/LoRa/M2M100/\"\n",
    ")\n",
    "\n",
    "# Evaluate all models\n",
    "untrained_bleu, untrained_loss = evaluate_model(untrained_model, test_dataset)\n",
    "fine_tuned_bleu, fine_tuned_loss = evaluate_model(fine_tuned_model, test_dataset)\n",
    "lora_bleu, lora_loss = evaluate_model(lora_model, test_dataset)\n",
    "\n",
    "# Plot BLEU scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar([\"Untrained\", \"Fine-Tuned\", \"LoRA\"], [untrained_bleu, fine_tuned_bleu, lora_bleu], color=[\"blue\", \"green\", \"orange\"])\n",
    "plt.ylabel(\"BLEU Score\")\n",
    "plt.title(\"BLEU Score Comparison\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar([\"Untrained\", \"Fine-Tuned\", \"LoRA\"], [untrained_loss, fine_tuned_loss, lora_loss], color=[\"blue\", \"green\", \"orange\"])\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Saving Desire Model to Hugging Face library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b3d0858bd14986a74750ff08a466b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WPanda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
